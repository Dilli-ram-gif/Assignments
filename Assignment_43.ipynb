{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9168eec6",
   "metadata": {},
   "source": [
    "### March 19, Feature Extraction Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5794b44a",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0de50c",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features into a common range. It rescales the values of a feature to a fixed range, typically between 0 and 1. \n",
    "\n",
    "- Min-Max scaling is calculated using the formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where min_value and max_value are the minimum and maximum values of the feature, respectively.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Let's say we have a dataset with a feature representing the age of individuals, ranging from 20 to 60. We can apply Min-Max scaling to transform the values into a range between 0 and 1. If we have an individual with an age of 30, the Min-Max scaling formula will give us:\n",
    "\n",
    "scaled_value = (30 - 20) / (60 - 20) = 0.25\n",
    "\n",
    "So the scaled value for an age of 30 would be 0.25 after applying Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bdec0a",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb195941",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method that rescales the values of a feature to have a unit norm or length. Unlike Min-Max scaling, the Unit Vector technique does not map the values to a specific range.\n",
    "\n",
    "The Unit Vector technique is calculated using the formula:\n",
    "\n",
    "scaled_value = value / norm\n",
    "\n",
    "where value is the original value of the feature, and norm is the Euclidean norm or length of the feature vector.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider a dataset with a feature representing the height of individuals. The values range from 150 cm to 180 cm. We can apply the Unit Vector technique to normalize the values. Let's say we have an individual with a height of 165 cm. The Euclidean norm of the feature vector is calculated as:\n",
    "\n",
    "norm = sqrt((150^2) + (180^2)) ≈ 228.04\n",
    "\n",
    "Using the Unit Vector formula, the scaled value for a height of 165 cm would be:\n",
    "\n",
    "scaled_value = 165 / 228.04 ≈ 0.723\n",
    "So the scaled value for a height of 165 cm would be approximately 0.723 after applying the Unit Vector technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557a3f8",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba8c0e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving the most important information in the data. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "- PCA works by calculating the eigenvectors and eigenvalues of the data covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each component. By selecting a subset of the principal components with the highest eigenvalues, we can reduce the dimensionality of the data while retaining most of the information.\n",
    "Example:\n",
    "- Let us  say we have a dataset with several numerical features representing financial metrics of companies, such as revenue, profit, and expenses. The original dataset has, for instance, 100 features. We can apply PCA to reduce the dimensionality and represent the data with, let's say, 10 principal components. These principal components are linear combinations of the original features, and each component explains a certain amount of variance in the data. By using only the top 10 components, we can represent the data in a lower-dimensional space while preserving most of the important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed0001f",
   "metadata": {},
   "source": [
    "##### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63381f9",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- PCA can be used for feature extraction by transforming the original features into a new set of features, the principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they capture. By selecting a subset of the principal components, we can create a reduced set of features that still contain most of the information from the original features.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider a dataset with images represented by pixel intensities. Each pixel can be considered a feature, resulting in high-dimensional data. By applying PCA to this dataset, we can extract a smaller set of features, the principal components, that capture the most important information in the images. These principal components can then be used as features for further analysis or modeling. For instance, we can choose to retain the top 100 principal components and discard the rest, effectively reducing the dimensionality of the dataset from thousands of pixels to just 100 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef320ac3",
   "metadata": {},
   "source": [
    "##### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f7de7",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data. Let's say we have features such as price, rating, and delivery time. Min-Max scaling can be applied to each of these features individually to transform their values into a common range, such as 0 to 1. This ensures that all features are on a similar scale and avoids any potential dominance of a particular feature due to its magnitude. By applying Min-Max scaling, we can effectively compare and analyze the different features within the recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf9dcb",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02e2487",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "When working on a project to predict stock prices with a dataset containing multiple features, PCA can be used to reduce the dimensionality of the dataset. Instead of using all the original features, we can apply PCA to identify the principal components that capture the most significant variations in the data. By selecting a smaller number of principal components, we can reduce the dimensionality of the dataset while still retaining the essential information. This helps to avoid the curse of dimensionality and can improve the performance of the model by reducing noise and redundant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d93516",
   "metadata": {},
   "source": [
    "##### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a833ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new minimum and maximum values\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the minimum and maximum values of the original dataset\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\n",
    "\n",
    "# Print the scaled values\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed77b253",
   "metadata": {},
   "source": [
    "The values in the dataset have been transformed to a range of -1 to 1 using Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b935e2c",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa31f381",
   "metadata": {},
   "source": [
    " For feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of dimensionality reduction and the amount of information captured by each component. One common approach is to select the number of components that explain a certain percentage of the total variance in the data, such as 95% or 99%.\n",
    "\n",
    "To determine the number of principal components to retain, we can calculate the cumulative explained variance ratio, which represents the accumulated amount of variance explained by each principal component in descending order. We can then choose the number of components that explain a significant portion of the variance, such as reaching the desired percentage threshold.\n",
    "\n",
    "For example, if\n",
    "\n",
    "the cumulative explained variance ratio shows that the first three components explain 85% of the variance, we might choose to retain those three components. The choice of the number of components will depend on the trade-off between dimensionality reduction and preserving sufficient information for the specific analysis or modeling task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5cf34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ff27ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c83ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcfe30e1",
   "metadata": {},
   "source": [
    "### March 19, Feature Extraction Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c150387",
   "metadata": {},
   "source": [
    "#### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61585b5e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform numerical features into a common range. It rescales the values of a feature to a fixed range, typically between 0 and 1. \n",
    "\n",
    "- Min-Max scaling is calculated using the formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "where min_value and max_value are the minimum and maximum values of the feature, respectively.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Let's say we have a dataset with a feature representing the age of individuals, ranging from 20 to 60. We can apply Min-Max scaling to transform the values into a range between 0 and 1. If we have an individual with an age of 30, the Min-Max scaling formula will give us:\n",
    "\n",
    "scaled_value = (30 - 20) / (60 - 20) = 0.25\n",
    "\n",
    "So the scaled value for an age of 30 would be 0.25 after applying Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c95e",
   "metadata": {},
   "source": [
    "#### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d480cc",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The Unit Vector technique, also known as Unit Norm or Normalization, is a feature scaling method that scales the values of a feature to have a unit norm. It differs from Min-Max scaling in that it focuses on the direction or magnitude of the feature vector rather than its range. \n",
    "\n",
    "In Unit Vector scaling, each sample's feature values are divided by the Euclidean norm of the feature vector. The Euclidean norm (L2 norm) of a feature vector is computed as the square root of the sum of the squares of its elements. By dividing each element by the norm, the resulting feature vector has a unit norm, which means its length is equal to 1. This technique preserves the direction of the feature vector while reducing the scale of the values.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Suppose we have a dataset with a feature vector representing the quantities of three items: [apples, oranges, bananas]. The original feature vector is [3, 4, 5].\n",
    "\n",
    "1. Compute the Euclidean Norm: Calculate the Euclidean norm of the feature vector:\n",
    "\n",
    "   norm = sqrt(3^2 + 4^2 + 5^2) = sqrt(9 + 16 + 25) = sqrt(50) ≈ 7.071\n",
    "\n",
    "2. Apply Unit Vector Scaling: Divide each element of the feature vector by the norm:\n",
    "\n",
    "   scaled_feature_vector = [3/7.071, 4/7.071, 5/7.071] ≈ [0.424, 0.565, 0.707]\n",
    "\n",
    "The resulting feature vector [0.424, 0.565, 0.707] has a unit norm. The values are scaled down, preserving the direction or proportions of the original feature vector. This scaling can be particularly useful in situations where the magnitude of the feature values is not as important as their relative proportions or directions.\n",
    "\n",
    "In comparison, Min-Max scaling (also known as Normalization in some contexts) rescales the feature values to a specific range, typically between 0 and 1, based on the minimum and maximum values of the feature. It focuses on the range of the values rather than their directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ce906",
   "metadata": {},
   "source": [
    "#### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc45064b",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving the most important information in the data. It achieves this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "- PCA works by calculating the eigenvectors and eigenvalues of the data covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each component. By selecting a subset of the principal components with the highest eigenvalues, we can reduce the dimensionality of the data while retaining most of the information.\n",
    "Example:\n",
    "- Let us  say we have a dataset with several numerical features representing financial metrics of companies, such as revenue, profit, and expenses. The original dataset has, for instance, 100 features. We can apply PCA to reduce the dimensionality and represent the data with, let's say, 10 principal components. These principal components are linear combinations of the original features, and each component explains a certain amount of variance in the data. By using only the top 10 components, we can represent the data in a lower-dimensional space while preserving most of the important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b941e00",
   "metadata": {},
   "source": [
    "##### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88643dd9",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "- PCA can be used for feature extraction by transforming the original features into a new set of features, the principal components. These principal components are linear combinations of the original features and are ordered by the amount of variance they capture. By selecting a subset of the principal components, we can create a reduced set of features that still contain most of the information from the original features.\n",
    "\n",
    "- Example:\n",
    "\n",
    "Consider a dataset with images represented by pixel intensities. Each pixel can be considered a feature, resulting in high-dimensional data. By applying PCA to this dataset, we can extract a smaller set of features, the principal components, that capture the most important information in the images. These principal components can then be used as features for further analysis or modeling. For instance, we can choose to retain the top 100 principal components and discard the rest, effectively reducing the dimensionality of the dataset from thousands of pixels to just 100 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17517d",
   "metadata": {},
   "source": [
    "##### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68b348",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In the context of building a recommendation system for a food delivery service, Min-Max scaling can be used to preprocess the data with features such as price, rating, and delivery time. Here's how you can use Min-Max scaling:\n",
    "\n",
    "1. Understand the Range of Features: Start by understanding the range and distribution of each feature in the dataset. This will help you identify whether Min-Max scaling is appropriate for a particular feature.\n",
    "\n",
    "2. Determine the Range: Determine the minimum and maximum values for each feature. For example, the price feature might have a range of $5 to $30, the rating feature might have a range of 1 to 5, and the delivery time feature might have a range of 10 to 60 minutes.\n",
    "\n",
    "3. Apply Min-Max Scaling: Min-Max scaling rescales the values of each feature to a specified range, typically between 0 and 1. The formula for Min-Max scaling is as follows:\n",
    "\n",
    "   scaled_value = (original_value - min_value) / (max_value - min_value)\n",
    "\n",
    "   Apply this formula to each value of a feature to scale it within the desired range.\n",
    "\n",
    "4. Preprocessing the Dataset: Iterate through each feature and apply Min-Max scaling to rescale the values. You can either implement the scaling manually or use libraries such as scikit-learn in Python, which provide built-in functions for scaling.\n",
    "\n",
    "5. Interpretation: After scaling, all the features will have values between 0 and 1. This allows you to compare and combine the features more effectively without one feature dominating the others due to a larger range.\n",
    "\n",
    "6. Recommendation System: With the preprocessed data, you can build your recommendation system using appropriate algorithms such as collaborative filtering or content-based filtering. These algorithms will utilize the scaled features to generate recommendations based on user preferences, historical data, and other relevant factors.\n",
    "\n",
    "Min-Max scaling ensures that the values of different features are on a similar scale, preventing features with larger values from dominating the recommendation process. By normalizing the features within a specified range, the recommendation system can effectively consider all features in a balanced manner when generating recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90316d9d",
   "metadata": {},
   "source": [
    "#### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f030fec",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In the context of predicting stock prices using a dataset with multiple features, such as company financial data and market trends, PCA can be used to reduce the dimensionality of the dataset as follows:\n",
    "\n",
    "1. Preprocess the Dataset: Ensure that your dataset is properly preprocessed. Handle any missing values, perform feature scaling if needed, and encode categorical variables appropriately.\n",
    "\n",
    "2. Feature Selection: Before applying PCA, it is recommended to perform feature selection techniques to remove irrelevant or redundant features. This step helps to reduce noise and focus on the most informative features. Techniques like correlation analysis, mutual information, or domain knowledge can aid in selecting relevant features.\n",
    "\n",
    "3. Apply PCA: Once the dataset is preprocessed and feature selection is performed, you can proceed with applying PCA. PCA transforms the original dataset into a new set of uncorrelated variables, called principal components. These components are linear combinations of the original features and are ordered by the amount of variance they capture.\n",
    "\n",
    "4. Determine the Number of Components: To determine the number of components to retain, you can consider the explained variance ratio. The explained variance ratio represents the proportion of the total variance in the dataset that is explained by each principal component. You can plot the cumulative explained variance ratio and choose the number of components that capture a significant portion of the variance (e.g., 95%).\n",
    "\n",
    "5. Transform the Dataset: After determining the desired number of principal components, you can transform the dataset by selecting those components. This transformation results in a reduced-dimensional representation of the data, where each sample is represented by the selected principal components.\n",
    "\n",
    "6. Model Training and Evaluation: Finally, you can use the transformed dataset with reduced dimensionality to train and evaluate your stock price prediction model. Depending on the algorithm used, you may observe improved model performance due to the reduced dimensionality, reduced noise, and removal of multicollinearity.\n",
    "\n",
    "It's important to note that while PCA helps in dimensionality reduction, it may result in loss of interpretability since the transformed features are combinations of the original features. Additionally, for time-series data like stock prices, it's important to consider the temporal aspect and any inherent autocorrelation, which might require additional techniques tailored for time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88a965",
   "metadata": {},
   "source": [
    "##### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3983b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new minimum and maximum values\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the minimum and maximum values of the original dataset\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "\n",
    "# Perform Min-Max scaling\n",
    "scaled_data = (data - min_value) / (max_value - min_value) * (new_max - new_min) + new_min\n",
    "\n",
    "# Print the scaled values\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a053bc04",
   "metadata": {},
   "source": [
    "The values in the dataset have been transformed to a range of -1 to 1 using Min-Max scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4341e996",
   "metadata": {},
   "source": [
    "#### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68f922",
   "metadata": {},
   "source": [
    "#### Ans: \n",
    "- For feature extraction using PCA on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain depends on the desired level of dimensionality reduction and the amount of information captured by each component. One common approach is to select the number of components that explain a certain percentage of the total variance in the data, such as 95% or 99%.\n",
    "\n",
    "- To determine the number of principal components to retain, we can calculate the cumulative explained variance ratio, which represents the accumulated amount of variance explained by each principal component in descending order. We can then choose the number of components that explain a significant portion of the variance, such as reaching the desired percentage threshold.\n",
    "\n",
    "- For example, if the cumulative explained variance ratio shows that the first three components explain 85% of the variance, we might choose to retain those three components. The choice of the number of components will depend on the trade-off between dimensionality reduction and preserving sufficient information for the specific analysis or modeling task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

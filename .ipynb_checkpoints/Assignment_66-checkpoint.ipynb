{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f854b04",
   "metadata": {},
   "source": [
    "### April 20, KNN - I, Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa8834",
   "metadata": {},
   "source": [
    "#### Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16926d50",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The K-Nearest Neighbors (KNN) algorithm is a non-parametric machine learning algorithm used for both classification and regression tasks. In KNN, the prediction for a new data point is made based on its proximity to the training data.\n",
    "\n",
    "The basic idea behind KNN is to find the K nearest neighbors of a given data point in the feature space. The class or value of the data point is then determined by majority voting (for classification) or averaging (for regression) the labels or values of its K nearest neighbors. KNN assumes that similar data points are more likely to have similar labels or values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce355a1",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e112602",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The choice of K in KNN is a crucial hyperparameter that can significantly impact the performance of the algorithm. The selection of K depends on the characteristics of the dataset and the problem at hand. Here are a few considerations when choosing the value of K:\n",
    "\n",
    "- **Odd vs. Even K**: It is generally recommended to choose an odd value of K to avoid ties in majority voting. In case of ties, an even value of K may lead to arbitrary decision rules.\n",
    "\n",
    "- **Size of the Dataset**: For smaller datasets, it is advisable to choose a smaller value of K to prevent overfitting. As the dataset size increases, a larger value of K can be considered.\n",
    "\n",
    "- **Data Distribution**: The choice of K can be influenced by the underlying data distribution. For example, if the classes in a classification problem have distinct clusters, a smaller value of K may work well. However, if the classes are more spread out or overlapping, a larger value of K may be more suitable.\n",
    "\n",
    "- **Trade-off between Bias and Variance**: Smaller values of K tend to have low bias but high variance, which can result in overfitting. Larger values of K have low variance but high bias, potentially leading to underfitting. The choice of K involves finding a balance between bias and variance based on the dataset and problem complexity.\n",
    "\n",
    "It is common to perform model selection and hyperparameter tuning techniques, such as cross-validation or grid search, to find the optimal value of K that yields the best performance for a given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb8ffc",
   "metadata": {},
   "source": [
    "#### Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c7cc1e",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The difference between KNN classifier and KNN regressor lies in the nature of the target variable they predict:\n",
    "\n",
    "1. **KNN Classifier** : In KNN classification, the target variable is categorical or discrete, representing class labels. The goal is to assign a class label to a new data point based on the majority class of its K nearest neighbors. The class label with the highest frequency among the neighbors is assigned to the new data point.\n",
    "\n",
    "2. **KNN Regressor**: In KNN regression, the target variable is continuous or numerical, representing a value. The goal is to estimate the value of a new data point based on the average or weighted average of the values of its K nearest neighbors. The average value of the neighbors becomes the predicted value for the new data point.\n",
    "\n",
    "In both cases, the KNN algorithm uses the distance metric (such as Euclidean or Manhattan distance) to determine the similarity or proximity between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b4f1bf",
   "metadata": {},
   "source": [
    "#### Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56a27af",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The performance of the KNN algorithm can be evaluated using various evaluation metrics, depending on the type of problem being addressed (classification or regression). Here are some commonly used metrics:\n",
    "\n",
    "- Classification: For KNN classifiers, typical evaluation metrics include:\n",
    "\n",
    "- Accuracy: The proportion of correctly classified instances.\n",
    "\n",
    "- Precision: The proportion of true positive predictions out of all positive predictions. It measures the model's ability to avoid false positives.\n",
    "\n",
    "- Recall: The proportion of true positive predictions out of all actual positive instances. It measures the model's ability to identify all positive instances.\n",
    "\n",
    "- F1-score: The harmonic mean of precision and recall, providing a balanced measure of both metrics.\n",
    "\n",
    "- Regression: For KNN regressors, common evaluation metrics include:\n",
    "\n",
    "- Mean Squared Error (MSE): The average squared difference between the predicted and actual values. It penalizes large errors more than smaller ones.\n",
    "\n",
    "- Root Mean Squared Error (RMSE): The square root of MSE, which gives an interpretable measure in the same unit as the target variable.\n",
    "\n",
    "- Mean Absolute Error (MAE): The average absolute difference between the predicted and actual values. It provides a measure of average prediction error without squaring the differences.\n",
    "\n",
    "The choice of evaluation metric depends on the specific problem and the importance of different types of errors or performance criteria. Cross-validation techniques, such as k-fold cross-validation, can be employed to obtain more reliable estimates of the model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4d641",
   "metadata": {},
   "source": [
    "#### Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccde62",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the challenge faced by KNN (and other algorithms) when working with high-dimensional data. As the number of dimensions (features) increases, the available data becomes increasingly sparse, causing several issues:\n",
    "\n",
    "- **Increased Data Sparsity**: In high-dimensional spaces, the available data points are spread sparsely, making it difficult to find neighbors that are truly similar. The distance between data points becomes less informative as the number of dimensions increases.\n",
    "\n",
    "- **Decreased Discriminatory Power**: With a large number of dimensions, the differences between data points become less distinct, making it harder to discriminate between different classes or identify meaningful patterns. The curse of dimensionality can lead to reduced accuracy and increased uncertainty in predictions.\n",
    "\n",
    "- **Increased Computational Complexity**: Computing distances between data points becomes computationally expensive as the number of dimensions grows. The time and memory required for KNN increase exponentially with the number of dimensions.\n",
    "\n",
    "To address the curse of dimensionality in KNN, dimensionality reduction techniques, such as feature selection or feature extraction methods like Principal Component Analysis (PCA), can be applied to reduce the number of features while preserving important information. Additionally, data preprocessing techniques like scaling or normalization can help mitigate the impact of different scales in high-dimensional spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993eb5a3",
   "metadata": {},
   "source": [
    "#### Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0628a028",
   "metadata": {},
   "source": [
    "Dealing with missing values is an important consideration when using KNN. Here are a few strategies to handle missing values in KNN:\n",
    "\n",
    "- **Deletion**: If the dataset has a small number of missing values, one option is to delete the instances with missing values. However, this approach can lead to data loss and potential bias if the missing values are not missing completely at random.\n",
    "\n",
    "- **Imputation**: Another approach is to impute missing values with estimates based on the available data. In KNN, missing values can be imputed by assigning values based on the average or weighted average of the feature values of its nearest neighbors. The missing values are filled in using the values of the K nearest neighbors without considering the missing feature during the distance computation.\n",
    "\n",
    "- **Model-Based Imputation**: Alternatively, instead of imputing values based on the nearest neighbors, more sophisticated imputation techniques can be employed, such as regression imputation or imputation using other machine learning algorithms like decision trees or random forests.\n",
    "\n",
    "The choice of the imputation method depends on the specific dataset, the nature of the missing values, and the assumptions made about the missing data mechanism. It is important to carefully handle missing values to avoid bias or distortion in the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83784b",
   "metadata": {},
   "source": [
    "#### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21c375",
   "metadata": {},
   "source": [
    "The performance of KNN classifier and regressor depends on the nature of the problem and the characteristics of the data. Here's a comparison between the two:\n",
    "\n",
    "- **KNN Classifier**: KNN classifier is suitable for classification problems where the target variable is categorical or discrete. It works well when the decision boundary between classes is non-linear or complex. KNN classifier is robust to noise and can handle multi-class classification. However, it may struggle with imbalanced datasets and high-dimensional data due to the curse of dimensionality.\n",
    "\n",
    "- **KNN Regressor**: KNN regressor is suitable for regression problems where the target variable is continuous or numerical. It can capture non-linear relationships between features and the target variable. KNN regressor is robust to outliers and can handle multi-output regression tasks. However, it is sensitive to noise and outliers in the training data, and the choice of K can affect the bias-variance tradeoff.\n",
    "\n",
    "The selection between KNN classifier and regressor depends on the problem at hand. If the target variable is categorical and the goal is to classify instances into different classes, KNN classifier is appropriate. If the target variable is continuous and the objective is to predict a numerical value, KNN regressor is suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c75030",
   "metadata": {},
   "source": [
    "#### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa01700",
   "metadata": {},
   "source": [
    "- **Strengths of KNN**:\n",
    "\n",
    "KNN is simple and easy to understand, making it a good baseline algorithm.\n",
    "It can capture complex and non-linear relationships in the data.\n",
    "KNN doesn't make assumptions about the data distribution.\n",
    "KNN can handle multi-class classification and multi-output regression tasks.\n",
    "- **Weaknesses of KNN**:\n",
    "\n",
    "KNN can be computationally expensive, especially with large datasets and high-dimensional data.\n",
    "It is sensitive to the choice of K and the distance metric.\n",
    "KNN performs poorly with imbalanced datasets due to the majority voting approach.\n",
    "The curse of dimensionality affects KNN's performance in high-dimensional spaces.\n",
    "To address these weaknesses, the following techniques can be employed:\n",
    "\n",
    "Applying dimensionality reduction techniques to reduce the number of features and mitigate the curse of dimensionality.\n",
    "Using efficient data structures, such as KD-trees or Ball-trees, to speed up the search for nearest neighbors.\n",
    "Optimizing hyperparameters, such as the value of K and the distance metric, using cross-validation or grid search.\n",
    "Handling imbalanced datasets with techniques like oversampling, undersampling, or using class weights during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687d54d1",
   "metadata": {},
   "source": [
    "#### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6c0bd4",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN for measuring the similarity or dissimilarity between data points. Here are their differences:\n",
    "\n",
    "- **Euclidean Distance**: Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding coordinates of two points. In other words, it measures the length of the straight line connecting two points in a Cartesian coordinate system. Euclidean distance is influenced by the magnitudes of individual feature differences.\n",
    "\n",
    "- **Manhattan Distance**: Manhattan distance, also known as city block distance or L1 norm, is the sum of absolute differences between corresponding coordinates of two points. It measures the distance traveled along the grid lines of a city block, hence the name \"Manhattan.\" Manhattan distance is not influenced by the magnitudes of individual feature differences.\n",
    "The choice between Euclidean distance and Manhattan distance depends on the nature of the data and the problem. Euclidean distance is more sensitive to differences in magnitudes between features and is appropriate when the scaling of features is uniform. Manhattan distance, on the other hand, is less affected by differences in magnitudes and can be suitable when the features have different scales or when the problem requires a distance metric that emphasizes the differences along each coordinate axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f827e",
   "metadata": {},
   "source": [
    "#### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf0fbae",
   "metadata": {},
   "source": [
    "Feature scaling is an important preprocessing step in KNN to ensure that all features contribute equally to the distance calculations. Since KNN relies on the distances between data points, features with larger scales or ranges can dominate the distance calculations, potentially leading to biased results. Therefore, feature scaling is performed to bring all features to a similar scale.\n",
    "There are two common approaches for feature scaling in KNN:\n",
    "\n",
    "- **Normalization (Min-Max Scaling)**: In normalization, each feature is scaled to a specific range, typically between 0 and 1. The formula for normalization is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fde6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = (x - min(x)) / (max(x) - min(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869ae46b",
   "metadata": {},
   "source": [
    "This ensures that all feature values are transformed to the range [0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17a3031",
   "metadata": {},
   "source": [
    "- **Standardization (Z-score Scaling)**: In standardization, each feature is scaled to have zero mean and unit variance. The formula for standardization is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862faad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = (x - mean(x)) / std(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7709f0",
   "metadata": {},
   "source": [
    "This transforms the feature values to have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "By scaling the features, KNN can give equal importance to all features during the distance calculations, preventing any particular feature from dominating the results. It is important to apply the same scaling transformation to the test data as well to ensure consistent scaling across the dataset.\n",
    "\n",
    "Feature scaling is particularly beneficial when the features have different scales or units of measurement. However, it is not necessary when the features are already on a similar scale or when the distance metric used in KNN is not sensitive to the scales of features, such as Manhattan distance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

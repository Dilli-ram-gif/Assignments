{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd08fd99",
   "metadata": {},
   "source": [
    "### March 13, Advance Statistics - 6, Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1617c",
   "metadata": {},
   "source": [
    "#### Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb04c13",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "The assumptions required to use ANOVA are:\n",
    "\n",
    "1. Normality: The responses for each factor level have a normal population distribution.\n",
    "2. Homogeneity of variance: These distributions have the same variance.\n",
    "3. Independence: The data are independent.\n",
    "\n",
    "If these assumptions are not met, the validity of the results may be impacted. Violations of these assumptions can lead to incorrect conclusions and affect the accuracy of the ANOVA results.\n",
    "\n",
    "For example, if the normality assumption is violated, meaning that the data is not normally distributed, then the ANOVA results may not be accurate. Similarly, if the homogeneity of variance assumption is violated, meaning that the variances among groups are not equal, then the ANOVA results may not be accurate. Violations of the independence assumption can also lead to inaccurate ANOVA results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83889f32",
   "metadata": {},
   "source": [
    "#### Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b3cba",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "There are several types of ANOVA, but the three most common types are:\n",
    "\n",
    "- One-way ANOVA: This is used to compare the means of three or more groups to determine if there is a significant difference between them.\n",
    "- Two-way ANOVA: This is used to determine the interaction effects between two independent variables on a dependent variable.\n",
    "- Three-way ANOVA: This is used to determine the interaction effects between three independent variables on a dependent variable.\n",
    "\n",
    "Other types of ANOVA include nested ANOVA and repeated-measures ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea9801",
   "metadata": {},
   "source": [
    "#### Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6de186",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In ANOVA, the observed variance in a particular variable is partitioned into components attributable to different sources of variation. The partitioning of variance in ANOVA is important because it helps us understand the relative importance of different sources of variation in explaining the variability in the data.\n",
    "\n",
    "The partitioning of variance is done by calculating the sum of squares (SS) for each source of variation. The total sum of squares (SST) is then partitioned into two components: the sum of squares between groups (SSB) and the sum of squares within groups (SSW). The ratio of SSB to SSW is used to calculate the F-statistic, which is used to test for significant differences between group means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6ae84",
   "metadata": {},
   "source": [
    "#### Q4. How would you calculate the total sum of squares (SST), explain sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c16d09",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In a one-way ANOVA, the total sum of squares (SST) measures the total variation in the data, the explained sum of squares (SSE) measures the variation explained by the treatment or group, and the residual sum of squares (SSR) measures the variation that is not explained by the treatment or group.\n",
    "\n",
    "Here's how you can calculate SST, SSE, and SSR in a one-way ANOVA using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa62bcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST: 12.0\n",
      "SSE: 6.0\n",
      "SSR: 6.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# define the data\n",
    "group1 = np.array([3, 4, 5])\n",
    "group2 = np.array([2, 3, 4])\n",
    "group3 = np.array([1, 2, 3])\n",
    "data = np.concatenate([group1, group2, group3])\n",
    "\n",
    "# calculate the mean of the data\n",
    "mean = np.mean(data)\n",
    "\n",
    "# calculate SST\n",
    "SST = np.sum((data - mean) ** 2)\n",
    "\n",
    "# calculate the group means\n",
    "group_means = [np.mean(group1), np.mean(group2), np.mean(group3)]\n",
    "\n",
    "# calculate SSE\n",
    "SSE = np.sum((group1 - group_means[0]) ** 2) + \\\n",
    "      np.sum((group2 - group_means[1]) ** 2) + \\\n",
    "      np.sum((group3 - group_means[2]) ** 2)\n",
    "\n",
    "# calculate the degrees of freedom\n",
    "df_between = 2  # number of groups minus 1\n",
    "df_within = 6   # total number of observations minus number of groups\n",
    "\n",
    "# calculate MSR and MSW\n",
    "MSR = SSE / df_between\n",
    "MSW = SST / df_within\n",
    "\n",
    "# calculate F-statistic and p-value\n",
    "F = MSR / MSW\n",
    "p_value = 1 - stats.f.cdf(F, df_between, df_within)\n",
    "\n",
    "# calculate SSR\n",
    "SSR = SST - SSE\n",
    "\n",
    "# print results\n",
    "print(f\"SST: {SST}\")\n",
    "print(f\"SSE: {SSE}\")\n",
    "print(f\"SSR: {SSR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9710340",
   "metadata": {},
   "source": [
    "#### Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98131f8a",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In a two-way ANOVA, there are two factors or independent variables, and the response variable is continuous. The main effects refer to the effects of each factor separately, while the interaction effect refers to the effect of the two factors together.\n",
    "\n",
    "Here's how you can calculate the main effects and interaction effects using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d773e671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main effect 1: 0.190\n",
      "Main effect 2: 0.048\n",
      "Interaction effect: 0.000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# define the data\n",
    "data = pd.DataFrame({\n",
    "    'Factor1': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],\n",
    "    'Factor2': ['X', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "    'Response': [10, 15, 20, 25, 30, 35, 40, 45]\n",
    "})\n",
    "\n",
    "# fit the two-way ANOVA model\n",
    "model = ols('Response ~ Factor1 + Factor2 + Factor1:Factor2', data=data).fit()\n",
    "table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "# extract the main effects and interaction effect\n",
    "main_effect1 = table.loc['Factor1', 'sum_sq'] / table['sum_sq'].sum()\n",
    "main_effect2 = table.loc['Factor2', 'sum_sq'] / table['sum_sq'].sum()\n",
    "interaction_effect = table.loc['Factor1:Factor2', 'sum_sq'] / table['sum_sq'].sum()\n",
    "\n",
    "# print the results\n",
    "print(f\"Main effect 1: {main_effect1:.3f}\")\n",
    "print(f\"Main effect 2: {main_effect2:.3f}\")\n",
    "print(f\"Interaction effect: {interaction_effect:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11ed94c",
   "metadata": {},
   "source": [
    "#### Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d554c3ae",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In this case, the F-statistic is 5.23 and the p-value is 0.02. The p-value represents the probability of obtaining a test statistic as extreme as the observed one, assuming the null hypothesis is true. A p-value of 0.02 indicates that the probability of observing such a large F-statistic by chance alone is only 2%. This suggests that there is strong evidence to reject the null hypothesis and conclude that there are significant differences between the means of the groups.\n",
    "\n",
    "The F-statistic provides a measure of how much the variation between the groups exceeds the variation within the groups. A large F-statistic indicates that the variation between the groups is much larger than the variation within the groups, which is consistent with the alternative hypothesis. In contrast, a small F-statistic would suggest that the variation between the groups is similar to the variation within the groups, which would support the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e5f35",
   "metadata": {},
   "source": [
    "#### Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f084a",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "In a repeated measures ANOVA, missing data can occur when one or more observations are missing for a particular subject or when a subject is missing entirely. There are several methods to handle missing data in repeated measures ANOVA, including:\n",
    "\n",
    "1. Complete Case Analysis (CCA): This approach involves only using data from subjects who have complete data on all variables. However, this method can lead to a loss of statistical power if there are a large number of missing observations.\n",
    "\n",
    "2. Pairwise Deletion (PWD): This approach involves using all available data for each analysis, with missing values being excluded on a pairwise basis. This method can be useful if the amount of missing data is small, but it can lead to biased estimates and reduced power if the missing data is not missing completely at random (MCAR).\n",
    "\n",
    "3. Mean Imputation: This approach involves replacing missing values with the mean value of the non-missing observations. This method can lead to biased estimates and a loss of statistical power, as it assumes that the missing values are equal to the mean of the non-missing values.\n",
    "\n",
    "4. Maximum Likelihood (ML) or Expectation-Maximization (EM) methods: These methods are based on modeling the data using a probability distribution and estimating the missing data values based on the observed data. They can produce unbiased estimates and can handle data that is not missing completely at random, but they can be computationally intensive and may require a large sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e179aeed",
   "metadata": {},
   "source": [
    "#### Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb8d64",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Post-hoc tests are used to identify which groups are significantly different from each other after obtaining a significant result in ANOVA. Some common post-hoc tests include:\n",
    "\n",
    "1. Tukey's HSD (Honestly Significant Difference) Test: This test is used when comparing all possible pairs of groups. It is a conservative test that controls for Type I error rates and is appropriate when the number of groups being compared is small.\n",
    "\n",
    "2. Bonferroni Test: This test is used when comparing all possible pairs of groups and controls for Type I error rates. It is more conservative than Tukey's test and is appropriate when the number of groups being compared is large.\n",
    "\n",
    "3. Dunnett's Test: This test is used when comparing each group to a control group. It controls for Type I error rates and is appropriate when there is a control group and a small number of treatment groups.\n",
    "\n",
    "4. Scheffe's Test: This test is used when comparing all possible pairs of groups and controls for Type I error rates. It is more conservative than Tukey's test and Bonferroni's test and is appropriate when the number of groups being compared is large and the sample sizes are unequal.\n",
    "\n",
    "5. Fisher's LSD (Least Significant Difference) Test: This test is used when comparing all possible pairs of groups and does not control for Type I error rates. It is appropriate when the sample sizes are equal and the variances are homogeneous.\n",
    "\n",
    "A situation where a post-hoc test might be necessary is when conducting an experiment with multiple treatment groups and a significant difference is found in the overall ANOVA test. For example, suppose a study is conducted to compare the effectiveness of four different medications for treating a particular condition. If the overall ANOVA test indicates a significant difference among the four groups, a post-hoc test can be used to determine which groups are significantly different from each other in terms of effectiveness. In this case, Tukey's HSD or Bonferroni's test might be appropriate as they control for Type I error rates and can be used to compare all possible pairs of groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641e748b",
   "metadata": {},
   "source": [
    "#### Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af09530",
   "metadata": {},
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
